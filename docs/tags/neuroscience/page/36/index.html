<!DOCTYPE html>
<html><head>
  
  <title>Neuroscience | audiences</title>
  
  <link rel="stylesheet" href='https://carjed.github.io/audiences/css/bootstrap.min.css'>
  <link rel="stylesheet" href='https://carjed.github.io/audiences/css/mondrian.css'>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.52" />
  

<style>

  #grid .list-item.page:hover {
    background: white;
  }
  main.single.page h1 {
    border-color: white;
  }

  #grid .list-item.reports:hover {
    background: gold;
  }
  main.single.reports h1 {
    border-color: gold;
  }

  #grid .list-item.summary:hover {
    background: limegreen;
  }
  main.single.summary h1 {
    border-color: limegreen;
  }

</style>
  









<script src="https://carjed.github.io/audiences/js/bundle.min.f25ff27688046eb318bdc22977150cde493b69297eb61c33d467745adf840d358a2427ed01d7dc1e0bd54c4d5831f1ed5e9bc1e1f7187533a24e36d63659607f.js" integrity="sha512-8l/ydogEbrMYvcIpdxUM3kk7aSl&#43;thwz1Gd0Wt&#43;EDTWKJCftAdfcHgvVTE1YMfHtXpvB4fcYdTOiTjbWNllgfw=="></script>

  <style></style>
  
  
</head>
<body>
  <div class="container-fluid h-100 d-flex flex-column">
<header class="row flex-shrink-0 justify-content-center justify-content-md-start">
  <ul class="nav px-2">
    <li class="p-2">
      <a class="text-muted text-decoration-none" href="../../../../">audiences</a>
    </li>
    
      <li class="p-2">
        <a class="text-light" href="reports" title="">reports</a>
      </li>
    
      <li class="p-2">
        <a class="text-light" href="search" title="">search</a>
      </li>
    
  </nav>
</header>

<main id="grid" class="row list flex-wrap h-99 text-center">
  
    <div class="list-item d-flex lead justify-content-center align-items-center reports">
      <div>
        <a class="text-decoration-none" href="https://carjed.github.io/audiences/reports/towards_reconstructing_intelligible_speech_from_the_human_auditory_cortex_43884843/">
          <h4 class="font-weight-bold">Towards reconstructing intelligible speech from the human auditory cortex, bioRxiv, 2018-06-19</h4>
          <p>AbstractAuditory stimulus reconstruction is a technique that finds the best approximation of the acoustic stimulus from the population of evoked neural activity. Reconstructing speech from the human auditory cortex creates the possibility of a speech neuroprosthetic to establish a direct communication with the brain and has been shown to be possible in both overt and covert conditions. However, the low quality of the reconstructed speech has severely limited the utility of this method for brain-computer interface (BCI) applications. To advance the state-of-the-art in speech neuroprosthesis, we combined the recent advances in deep learning with the latest innovations in speech synthesis technologies to reconstruct closed-set intelligible speech from the human auditory cortex. We investigated the dependence of reconstruction accuracy on linear and nonlinear (deep neural network) regression methods and the acoustic representation that is used as the target of reconstruction, including auditory spectrogram and speech synthesis parameters. In addition, we compared the reconstruction accuracy from low and high neural frequency ranges. Our results show that a deep neural network model that directly estimates the parameters of a speech synthesizer from all neural frequencies achieves the highest subjective and objective scores on a digit recognition task, improving the intelligibility by 65% over the baseline method which used linear regression to reconstruct the auditory spectrogram. These results demonstrate the efficacy of deep learning and speech synthesis algorithms for designing the next generation of speech BCI systems, which not only can restore communications for paralyzed patients but also have the potential to transform human-computer interaction technologies.</p>
        </a>
        
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/biorxiv">biorxiv</a>
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/neuroscience">neuroscience</a>
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/0-100-users">0-100-users</a>
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/2018">2018</a>
          
        
      </div>
    </div>
  
    <div class="list-item d-flex lead justify-content-center align-items-center reports">
      <div>
        <a class="text-decoration-none" href="https://carjed.github.io/audiences/reports/calman_an_open_source_tool_for_scalable_calcium_imaging_data_analysis_43327142/">
          <h4 class="font-weight-bold">CalmAn An open source tool for scalable Calcium Imaging data Analysis, bioRxiv, 2018-06-05</h4>
          <p>AbstractAdvances in fluorescence microscopy enable monitoring larger brain areas in-vivo with finer time resolution. The resulting data rates require reproducible analysis pipelines that are reliable, fully automated, and scalable to datasets generated over the course of months. Here we present CaImAn, an open-source library for calcium imaging data analysis. CaImAn provides automatic and scalable methods to address problems common to pre-processing, including motion correction, neural activity identification, and registration across different sessions of data collection. It does this while requiring minimal user intervention, with good performance on computers ranging from laptops to high-performance computing clusters. CaImAn is suitable for two-photon and one-photon imaging, and also enables real-time analysis on streaming data. To benchmark the performance of CaImAn we collected a corpus of ground truth annotations from multiple labelers on nine mouse two-photon datasets. We demonstrate that CaImAn achieves near-human performance in detecting locations of active neurons.</p>
        </a>
        
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/biorxiv">biorxiv</a>
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/neuroscience">neuroscience</a>
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/0-100-users">0-100-users</a>
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/2018">2018</a>
          
        
      </div>
    </div>
  
    <div class="list-item d-flex lead justify-content-center align-items-center reports">
      <div>
        <a class="text-decoration-none" href="https://carjed.github.io/audiences/reports/a_genetically_encoded_fluorescent_sensor_for_in_vivo_imaging_of_gaba_41457576/">
          <h4 class="font-weight-bold">A genetically encoded fluorescent sensor for in vivo imaging of GABA, bioRxiv, 2018-05-15</h4>
          <p>AbstractCurrent techniques for monitoring GABA, the primary inhibitory neurotransmitter in vertebrates, cannot follow ephemeral transients in intact neural circuits. We applied the design principles used to create iGluSnFR, a fluorescent reporter of synaptic glutamate, to develop a GABA sensor using a protein derived from a previously unsequenced Pseudomonas fluorescens strain. Structure-guided mutagenesis and library screening led to a usable iGABASnFR (ΔFFmax ~ 2.5, Kd ~ 9 μM, good specificity, adequate kinetics). iGABASnFR is genetically encoded, detects single action potential-evoked GABA release events in culture, and produces readily detectable fluorescence increases in vivo in mice and zebrafish. iGABASnFR enabled tracking of (1) mitochondrial GABA content and its modulation by an anticonvulsant; (2) swimming-evoked GABAergic transmission in zebrafish cerebellum; (3) GABA release events during inter-ictal spikes and seizures in awake mice; and (4) GABAergic tone decreases during isoflurane anesthesia. iGABASnFR will permit high spatiotemporal resolution of GABA signaling in intact preparations.</p>
        </a>
        
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/biorxiv">biorxiv</a>
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/neuroscience">neuroscience</a>
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/100-200-users">100-200-users</a>
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/2018">2018</a>
          
        
      </div>
    </div>
  
    <div class="list-item d-flex lead justify-content-center align-items-center reports">
      <div>
        <a class="text-decoration-none" href="https://carjed.github.io/audiences/reports/single-trial_neural_dynamics_are_dominated_by_richly_varied_movements_39309310/">
          <h4 class="font-weight-bold">Single-trial neural dynamics are dominated by richly varied movements, bioRxiv, 2018-04-25</h4>
          <p>When experts are immersed in a task, do their brains prioritize task-related activity? Most efforts to understand neural activity during well-learned tasks focus on cognitive computations and specific task-related movements. We wondered whether task-performing animals explore a broader movement landscape, and how this impacts neural activity. We characterized movements using video and other sensors and measured neural activity using widefield and two-photon imaging. Cortex-wide activity was dominated by movements, especially uninstructed movements, reflecting unknown priorities of the animal. Some uninstructed movements were aligned to trial events. Accounting for them revealed that neurons with similar trial-averaged activity often reflected utterly different combinations of cognitive and movement variables. Other movements occurred idiosyncratically, accounting for trial-by-trial fluctuations that are often considered “noise”. This held true for extracellular Neuropixels recordings in cortical and subcortical areas. Our observations argue that animals execute expert decisions while performing richly varied, uninstructed movements that profoundly shape neural activity.</p>
        </a>
        
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/biorxiv">biorxiv</a>
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/neuroscience">neuroscience</a>
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/200-500-users">200-500-users</a>
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/2018">2018</a>
          
        
      </div>
    </div>
  
    <div class="list-item d-flex lead justify-content-center align-items-center reports">
      <div>
        <a class="text-decoration-none" href="https://carjed.github.io/audiences/reports/spontaneous_behaviors_drive_multidimensional_brain-wide_activity_38736382/">
          <h4 class="font-weight-bold">Spontaneous behaviors drive multidimensional, brain-wide activity, bioRxiv, 2018-04-22</h4>
          <p>Cortical responses to sensory stimuli are highly variable, and sensory cortex exhibits intricate spontaneous activity even without external sensory input. Cortical variability and spontaneous activity have been variously proposed to represent random noise, recall of prior experience, or encoding of ongoing behavioral and cognitive variables. Here, by recording over 10,000 neurons in mouse visual cortex, we show that spontaneous activity reliably encodes a high-dimensional latent state, which is partially related to the mouse’s ongoing behavior and is represented not just in visual cortex but across the forebrain. Sensory inputs do not interrupt this ongoing signal, but add onto it a representation of visual stimuli in orthogonal dimensions. Thus, visual cortical population activity, despite its apparently noisy structure, reliably encodes an orthogonal fusion of sensory and multidimensional behavioral information.</p>
        </a>
        
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/biorxiv">biorxiv</a>
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/neuroscience">neuroscience</a>
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/200-500-users">200-500-users</a>
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/2018">2018</a>
          
        
      </div>
    </div>
  
    <div class="list-item d-flex lead justify-content-center align-items-center reports">
      <div>
        <a class="text-decoration-none" href="https://carjed.github.io/audiences/reports/parameterizing_neural_power_spectra_36573180/">
          <h4 class="font-weight-bold">Parameterizing neural power spectra, bioRxiv, 2018-04-11</h4>
          <p>AbstractElectrophysiological signals across species and recording scales exhibit both periodic and aperiodic features. Periodic oscillations have been widely studied and linked to numerous physiological, cognitive, behavioral, and disease states, while the aperiodic “background” 1f component of neural power spectra has received far less attention. Most analyses of oscillations are conducted on a priori, canonically-defined frequency bands without consideration of the underlying aperiodic structure, or verification that a periodic signal even exists in addition to the aperiodic signal. This is problematic, as recent evidence shows that the aperiodic signal is dynamic, changing with age, task demands, and cognitive state. It has also been linked to the relative excitationinhibition of the underlying neuronal population. This means that standard analytic approaches easily conflate changes in the periodic and aperiodic signals with one another because the aperiodic parameters—along with oscillation center frequency, power, and bandwidth—are all dynamic in physiologically meaningful, but likely different, ways. In order to overcome the limitations of traditional narrowband analyses and to reduce the potentially deleterious effects of conflating these features, we introduce a novel algorithm for automatic parameterization of neural power spectral densities (PSDs) as a combination of the aperiodic signal and putative periodic oscillations. Notably, this algorithm requires no a priori specification of band limits and accounts for potentially-overlapping oscillations while minimizing the degree to which they are confounded with one another. This algorithm is amenable to large-scale data exploration and analysis, providing researchers with a tool to quickly and accurately parameterize neural power spectra.</p>
        </a>
        
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/biorxiv">biorxiv</a>
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/neuroscience">neuroscience</a>
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/200-500-users">200-500-users</a>
          
            <a class="text-body text-decoration-none border-bottom border-dark" href="../../../../tags/2018">2018</a>
          
        
      </div>
    </div>
  
</main>
<br>
<main>
  


<nav aria-label="page navigation">
    <ul class="pagination">
        
        
        <li class="page-item"><a href="../../../../tags/neuroscience/" rel="first" class="page-link">« First</a></li>
        

        
        <li class="page-item"><a href="../../../../tags/neuroscience/page/35/" rel="prev" class="page-link">‹ Prev</a></li>
        

        
             
            
            <li class="page-item disabled"><a class="page-link">...</a></li>
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
             
            <li class="page-item"><a href="../../../../tags/neuroscience/page/32/" class="page-link">32</a></li>
            
        
            
             
            <li class="page-item"><a href="../../../../tags/neuroscience/page/33/" class="page-link">33</a></li>
            
        
            
             
            <li class="page-item"><a href="../../../../tags/neuroscience/page/34/" class="page-link">34</a></li>
            
        
            
             
            <li class="page-item"><a href="../../../../tags/neuroscience/page/35/" class="page-link">35</a></li>
            
        
             
            <li class="page-item active"><a href="../../../../tags/neuroscience/page/36/" class="page-link">36</a></li>
            
        
            
             
            <li class="page-item"><a href="../../../../tags/neuroscience/page/37/" class="page-link">37</a></li>
            
        
            
             
            <li class="page-item"><a href="../../../../tags/neuroscience/page/38/" class="page-link">38</a></li>
            
        
            
             
            <li class="page-item"><a href="../../../../tags/neuroscience/page/39/" class="page-link">39</a></li>
            
        
            
             
            <li class="page-item"><a href="../../../../tags/neuroscience/page/40/" class="page-link">40</a></li>
            
        
             
            
            <li class="page-item disabled"><a class="page-link">...</a></li>
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
        

        
        <li class="page-item"><a href="../../../../tags/neuroscience/page/37/" rel="next" class="page-link">Next ›</a></li>
        

        
        
        <li class="page-item"><a href="../../../../tags/neuroscience/page/53/" rel="last" class="page-link">Last »</a></li>
        
    </ul>
</nav>


</main>

&nbsp;
<hr />
<p style="text-align: center;">Created with the <a href="https://github.com/carjed/audiences">audiences framework </a> by <a href="https://github.com/carjed/">Jedidiah Carlson</a></p>
<p style="text-align: center;">Powered by <a href="https://gohugo.io/">Hugo </a></p>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


<p style="text-align: center;">
    <a href="https://twitter.com/JedMSP?lang=en" class="fa fa-twitter"></a>
    <a href="https://github.com/carjed/" class="fa fa-github"></a>
</p>

&nbsp;

</div>
  
</body>
</html>
